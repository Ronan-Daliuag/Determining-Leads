{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"feature_engineering.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['ID'])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also encode Education and Marital_Status\n",
    "features = pd.get_dummies(features, columns=['Education'], prefix='Education')\n",
    "features = pd.get_dummies(features, columns=['Marital_Status'], prefix='Marital')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(features.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X = features.drop(columns=['Response'], axis=1).iloc[:,1:][['Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', \n",
    "#                                                  'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', \n",
    "#                                                  'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', \n",
    "#                                                  'NumStorePurchases', 'NumWebVisitsMonth', 'Complain', \n",
    "#                                                  'Age', 'Customer_Enrollment', 'Household', 'Total_Spent', \n",
    "#                                                  'Total_Purchases', 'Total_Visits', 'Average_Purchase', \n",
    "#                                                  'Education_2n Cycle', 'Education_Basic', 'Education_Graduation', \n",
    "#                                                  'Education_Master', 'Education_PhD', 'Marital_Divorced', \n",
    "#                                                  'Marital_Married', 'Marital_Single', 'Marital_Together', 'Marital_Widow',\n",
    "#                                                  'Income']]\n",
    "X = features.drop(columns=['Response'], axis=1).iloc[:,1:][['Complain', \n",
    "                                                 'Age', 'Customer_Enrollment', 'Household', 'Total_Spent', \n",
    "                                                 'Total_Purchases', 'Total_Visits', 'Average_Purchase', \n",
    "                                                 'Education_2n Cycle', 'Education_Basic', 'Education_Graduation', \n",
    "                                                 'Education_Master', 'Education_PhD', 'Marital_Divorced', \n",
    "                                                 'Marital_Married', 'Marital_Single', 'Marital_Together', 'Marital_Widow',\n",
    "                                                 'Income']]\n",
    "# X = features.drop(columns=['Response'], axis = 1)\n",
    "y = features['Response']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "X_train.shape, X_test.shape\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'penalty':['l1','l2']}, \n",
    "              {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "grid_search = GridSearchCV(estimator = lg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'precision',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "lg = grid_search.best_estimator_\n",
    "lg.fit(X_train_scaled, y_train)\n",
    "lg_pred_train = lg.predict(X_train_scaled)\n",
    "lg_pred_test = lg.predict(X_test_scaled)\n",
    "lg_acc = accuracy_score(y_test, lg_pred_test)\n",
    "lg_prec = precision_score(y_test, lg_pred_test)\n",
    "lg_rec = recall_score(y_test, lg_pred_test)\n",
    "lg_f1 = f1_score(y_test, lg_pred_test)\n",
    "lg_auc = roc_auc_score(y_test, lg_pred_test)\n",
    "print(\"Logistic Regression Accuracy: %.4f\" % lg_acc)\n",
    "print(\"Logistic Regression Precision: %.4f\" % lg_prec)\n",
    "print(\"Logistic Regression Recall: %.4f\" % lg_rec)\n",
    "print(\"Logistic Regression F1: %.4f\" % lg_f1)\n",
    "print(\"Logistic Regression AUC: %.4f\" % lg_auc)\n",
    "\n",
    "lg_prec_train = precision_score(y_train, lg_pred_train)\n",
    "lg_prec_test = precision_score(y_test, lg_pred_test)\n",
    "# Check for overfitting\n",
    "print('Training set score: {:.4f}'.format(lg_prec_train))\n",
    "print('Test set score: {:.4f}'.format(lg_prec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lg, X_train_scaled, y_train, cv = 5, scoring='precision')\n",
    "print('Cross-validation scores:{}'.format(scores))\n",
    "print('Average cross-validation score: {:.4f}'.format(scores.mean())) # lower CV score = does not result in improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "lg_cm = confusion_matrix(y_test, lg_pred_test)\n",
    "print('Confusion matrix\\n\\n', lg_cm)\n",
    "print('\\nTrue Positives(TP) = ', lg_cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', lg_cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', lg_cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', lg_cm[1,0])\n",
    "cm_matrix = pd.DataFrame(data=lg_cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                                 index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pred_train = nb.predict(X_train)\n",
    "nb_pred_test = nb.predict(X_test)\n",
    "nb_acc = accuracy_score(y_test, nb_pred_test)\n",
    "nb_prec = precision_score(y_test, nb_pred_test)\n",
    "nb_rec = recall_score(y_test, nb_pred_test)\n",
    "nb_f1 = f1_score(y_test, nb_pred_test)\n",
    "nb_auc = roc_auc_score(y_test, nb_pred_test)\n",
    "print(\"Naive Bayes Accuracy: %.4f\" % nb_acc)\n",
    "print(\"Naive Bayes Precision: %.4f\" % nb_prec)\n",
    "print(\"Naive Bayes Recall: %.4f\" % nb_rec)\n",
    "print(\"Naive Bayes F1: %.4f\" % nb_f1)\n",
    "print(\"Naive Bayes AUC: %.4f\" % nb_auc)\n",
    "\n",
    "nb_prec_train = precision_score(y_train, nb_pred_train)\n",
    "nb_prec_test = precision_score(y_test, nb_pred_test)\n",
    "# Check for overfitting\n",
    "print('Training set score: {:.4f}'.format(nb_prec_train))\n",
    "print('Test set score: {:.4f}'.format(nb_prec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(nb, X_train, y_train, cv = 5, scoring='precision')\n",
    "print('Cross-validation scores:{}'.format(scores))\n",
    "print('Average cross-validation score: {:.4f}'.format(scores.mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cm = confusion_matrix(y_test, nb_pred_test)\n",
    "print('Confusion matrix\\n\\n', nb_cm)\n",
    "print('\\nTrue Positives(TP) = ', nb_cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', nb_cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', nb_cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', nb_cm[1,0])\n",
    "cm_matrix = pd.DataFrame(data=nb_cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                                 index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'],  # Splitting criterion\n",
    "    'max_depth': [None, 3, 5, 7],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = dt,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'precision',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = grid_search.best_estimator_\n",
    "dt.fit(X_train,y_train)\n",
    "dt_pred_train = dt.predict(X_train)\n",
    "dt_pred_test = dt.predict(X_test)\n",
    "dt_acc = accuracy_score(y_test, dt_pred_test)\n",
    "dt_prec = precision_score(y_test, dt_pred_test)\n",
    "dt_rec = recall_score(y_test, dt_pred_test)\n",
    "dt_f1 = f1_score(y_test, dt_pred_test)\n",
    "dt_auc = roc_auc_score(y_test, dt_pred_test)\n",
    "print(\"Decision Trees Accuracy: %.4f\" % dt_acc)\n",
    "print(\"Decision Trees Precision: %.4f\" % dt_prec)\n",
    "print(\"Decision Trees Recall: %.4f\" % dt_rec)\n",
    "print(\"Decision Trees F1: %.4f\" % dt_f1)\n",
    "print(\"Decision Trees AUC: %.4f\" % dt_auc)\n",
    "\n",
    "dt_prec_train = precision_score(y_train, dt_pred_train)\n",
    "dt_prec_test = precision_score(y_test, dt_pred_test)\n",
    "# Check for overfitting\n",
    "print('Training set score: {:.4f}'.format(dt_prec_train))\n",
    "print('Test set score: {:.4f}'.format(dt_prec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cm = confusion_matrix(y_test, dt_pred_test)\n",
    "print('Confusion matrix\\n\\n', dt_cm)\n",
    "print('\\nTrue Positives(TP) = ', dt_cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', dt_cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', dt_cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', dt_cm[1,0])\n",
    "cm_matrix = pd.DataFrame(data=dt_cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                                 index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_n_neighbors(X, y, n_neighbors_range, cv=5, scoring='precision'):\n",
    "    mean_scores = []\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        scores = cross_val_score(knn, X, y, cv=cv, scoring=scoring)\n",
    "        mean_scores.append(np.mean(scores))\n",
    "    optimal_n_neighbors = n_neighbors_range[np.argmax(mean_scores)]\n",
    "    return optimal_n_neighbors, mean_scores\n",
    "optimal_n_neighbors, mean_scores = compute_optimal_n_neighbors(X_train_scaled, y_train, n_neighbors_range=[3, 5, 7, 9, 11])\n",
    "print(\"Optimal number of neighbors:\", optimal_n_neighbors)\n",
    "print(\"Mean cross-validation scores:\", mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "parameters = {\n",
    "    'n_neighbors': [optimal_n_neighbors],  # Number of neighbors to consider\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'metric': ['euclidean', 'manhattan']  # Distance metric used for the tree\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = knn,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'precision',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = grid_search.best_estimator_\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "knn_pred_train = knn.predict(X_train_scaled)\n",
    "knn_pred_test = knn.predict(X_test_scaled)\n",
    "knn_acc = accuracy_score(y_test, knn_pred_test)\n",
    "knn_prec = precision_score(y_test, knn_pred_test)\n",
    "knn_rec = recall_score(y_test, knn_pred_test)\n",
    "knn_f1 = f1_score(y_test, knn_pred_test)\n",
    "knn_auc = roc_auc_score(y_test, knn_pred_test)\n",
    "print(\"k-Nearest Neighbors Accuracy: %.4f\" % knn_acc)\n",
    "print(\"k-Nearest Neighbors Precision: %.4f\" % knn_prec)\n",
    "print(\"k-Nearest Neighbors Recall: %.4f\" % knn_rec)\n",
    "print(\"k-Nearest Neighbors F1: %.4f\" % knn_f1)\n",
    "print(\"k-Nearest Neighbors AUC: %.4f\" % knn_auc)\n",
    "\n",
    "knn_prec_train = precision_score(y_train, knn_pred_train)\n",
    "knn_prec_test = precision_score(y_test, knn_pred_test)\n",
    "# Check for overfitting\n",
    "print('Training set score: {:.4f}'.format(knn_prec_train))\n",
    "print('Test set score: {:.4f}'.format(knn_prec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cm = confusion_matrix(y_test, knn_pred_test)\n",
    "print('Confusion matrix\\n\\n', knn_cm)\n",
    "print('\\nTrue Positives(TP) = ', knn_cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', knn_cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', knn_cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', knn_cm[1,0])\n",
    "cm_matrix = pd.DataFrame(data=knn_cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                                 index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm=SVC() \n",
    "svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "# param_grid = [ {'C':[1, 10, 100, 1000], 'kernel':['linear']},\n",
    "#                {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "#                {'C':[1, 10, 100, 1000], 'kernel':['poly'], 'degree': [2,3,4] ,'gamma':[0.01,0.02,0.03,0.04,0.05]} \n",
    "#               ]\n",
    "# grid_search = GridSearchCV(estimator = svm,  \n",
    "#                            param_grid = parameters,\n",
    "#                            scoring = 'recall',\n",
    "#                            cv = 5,\n",
    "#                            verbose=0)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "scorer = make_scorer(precision_score, average='binary', zero_division=0)\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100], \n",
    "#     'gamma': [1, 0.1, 0.01, 0.001], \n",
    "#     'kernel': ['linear', 'rbf'] \n",
    "# }\n",
    "grid_search = GridSearchCV(estimator=svm, scoring=scorer, param_grid=param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = grid_search.best_estimator_\n",
    "svm.fit(X_train_scaled,y_train)\n",
    "svm_pred_train = svm.predict(X_train_scaled)\n",
    "svm_pred_test = svm.predict(X_test_scaled)\n",
    "svm_acc = accuracy_score(y_test, svm_pred_test)\n",
    "svm_prec = precision_score(y_test, svm_pred_test)\n",
    "svm_rec = recall_score(y_test, svm_pred_test)\n",
    "svm_f1 = f1_score(y_test, svm_pred_test)\n",
    "svm_auc = roc_auc_score(y_test, svm_pred_test)\n",
    "print(\"Support Vector Machines Accuracy: %.4f\" % svm_acc)\n",
    "print(\"Support Vector Machines Precision: %.4f\" % svm_prec)\n",
    "print(\"Support Vector Machines Recall: %.4f\" % svm_rec)\n",
    "print(\"Support Vector Machines F1: %.4f\" % svm_f1)\n",
    "print(\"Support Vector Machines AUC: %.4f\" % svm_auc)\n",
    "\n",
    "svm_prec_train = precision_score(y_train, svm_pred_train)\n",
    "svm_prec_test = precision_score(y_test, svm_pred_test)\n",
    "# Check for overfitting\n",
    "print('Training set score: {:.4f}'.format(svm_prec_train))\n",
    "print('Test set score: {:.4f}'.format(svm_prec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cm = confusion_matrix(y_test, svm_pred_test)\n",
    "print('Confusion matrix\\n\\n', svm_cm)\n",
    "print('\\nTrue Positives(TP) = ', svm_cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', svm_cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', svm_cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', svm_cm[1,0])\n",
    "cm_matrix = pd.DataFrame(data=svm_cm, columns=['Actual Positive', 'Actual Negative'], \n",
    "                                 index=['Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------------------+-------------+---------------+---------------------+-------------------------+\n",
      "| Labels                   | Logistic Regression | Naive Bayes | Decision Tree | k-Nearest Neighbors | Support Vector Machines |\n",
      "+--------------------------+---------------------+-------------+---------------+---------------------+-------------------------+\n",
      "| Accuracy                 | 0.8631              | 0.7418      | 0.7903        | 0.8284              | 0.8752                  |\n",
      "| Precision                | 0.5091              | 0.2745      | 0.3186        | 0.3733              | 0.6250                  |\n",
      "| Recall                   | 0.3500              | 0.5250      | 0.4500        | 0.3500              | 0.2500                  |\n",
      "| F1                       | 0.4148              | 0.3605      | 0.3731        | 0.3613              | 0.3571                  |\n",
      "| AUC                      | 0.6478              | 0.6508      | 0.6475        | 0.6277              | 0.6129                  |\n",
      "| Precision Training Score | 0.9351              | 0.7260      | 0.9593        | 1.0000              | 0.9831                  |\n",
      "| Precision Test Score     | 0.5091              | 0.2745      | 0.3186        | 0.3733              | 0.6250                  |\n",
      "+--------------------------+---------------------+-------------+---------------+---------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC', 'Precision Training Score', 'Precision Test Score']\n",
    "\n",
    "logistic_reg = [lg_acc, lg_prec, lg_rec, lg_f1, lg_auc, lg_prec_train, lg_prec_test]\n",
    "logistic_reg = [f'{value:.4f}' if isinstance(value, float) else value for value in logistic_reg]\n",
    "\n",
    "naive_bayes = [nb_acc, nb_prec, nb_rec, nb_f1, nb_auc, nb_prec_train, nb_prec_test]\n",
    "naive_bayes = [f'{value:.4f}' if isinstance(value, float) else value for value in naive_bayes]\n",
    "\n",
    "decision_tree = [dt_acc, dt_prec, dt_rec, dt_f1, dt_auc, dt_prec_train, dt_prec_test]\n",
    "decision_tree = [f'{value:.4f}' if isinstance(value, float) else value for value in decision_tree]\n",
    "\n",
    "k_nearest = [knn_acc, knn_prec, knn_rec, knn_f1, knn_auc, knn_prec_train, knn_prec_test] \n",
    "k_nearest = [f'{value:.4f}' if isinstance(value, float) else value for value in k_nearest]\n",
    "\n",
    "support_vector = [svm_acc, svm_prec, svm_rec, svm_f1, svm_auc, svm_prec_train, svm_prec_test]\n",
    "support_vector = [f'{value:.4f}' if isinstance(value, float) else value for value in support_vector]\n",
    "\n",
    "# Create a list of tuples containing variable names and their values\n",
    "variables = [\n",
    "    ('Logistic Regression', logistic_reg),\n",
    "    ('Naive Bayes', naive_bayes),\n",
    "    ('Decision Tree', decision_tree),\n",
    "    ('k-Nearest Neighbors', k_nearest),\n",
    "    ('Support Vector Machines', support_vector)\n",
    "]\n",
    "table = PrettyTable()\n",
    "# Display variables in a tabular format\n",
    "table.add_column('Labels', labels)\n",
    "table.add_column('Logistic Regression', logistic_reg)\n",
    "table.add_column('Naive Bayes', naive_bayes)\n",
    "table.add_column('Decision Tree', decision_tree)\n",
    "table.add_column('k-Nearest Neighbors', k_nearest)\n",
    "table.add_column('Support Vector Machines', support_vector)\n",
    "\n",
    "table.align = 'l'\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
