1st Phase: All Data Cleaning done with RobustScaler as scaler for all but DecisionTrees
List of hyperparameters
Scalers: 0.2 Test Size, 0 Random state
Logistic Regression: solver = liblinear
Niave Bayes: none
DecisionTrees: criterion = entropy, max_depth = 3, RandomState = 0
kNN: neighbors = 3
SVM: C=1.0, kernel=rbf and gamma=auto, default parameters

Logistic Regression Accuracy: 0.8900
Logistic Regression Precision: 0.5714
Logistic Regression Recall: 0.2553
Logistic Regression F1: 0.3529
Logistic Regression AUC: 0.6149
Training set score: 0.8899
Test set score: 0.8900

Naive Bayes Accuracy: 0.7325
Naive Bayes Precision: 0.2222
Naive Bayes Recall: 0.5106
Naive Bayes F1: 0.3097
Naive Bayes AUC: 0.6363
Training set score: 0.7716
Test set score: 0.7325

Decision Trees Accuracy: 0.8725
Decision Trees Precision: 0.1667
Decision Trees Recall: 0.0213
Decision Trees F1: 0.0377
Decision Trees AUC: 0.5036
Training set score: 0.8723
Test set score: 0.8725

K-Nearest Neighbors Accuracy: 0.8700
K-Nearest Neighbors Precision: 0.3810
K-Nearest Neighbors Recall: 0.1702
K-Nearest Neighbors F1: 0.2353
K-Nearest Neighbors AUC: 0.5667
Training set score: 0.9193
Test set score: 0.8700

SVM Accuracy: 0.8875
SVM Precision: 0.6667
SVM Recall: 0.0851
SVM F1: 0.1509
SVM AUC: 0.5397
Training set score: 0.8949
Test set score: 0.8875

--------------------------------------------------------------------------------------------------------------------------------------
StandardScaler - GridSearchCV for best hyperparameters: Logistic Regression, SVM, kNN, and Decision Trees
Features: [Income, Household, Total_Spent, Total_Purchases, Total_Visits, Year of Enrollment, Education, Marital Status]
Logistic Regression Accuracy: 0.8475
Logistic Regression Precision: 0.5312
Logistic Regression Recall: 0.1889
Logistic Regression F1: 0.2787
Logistic Regression AUC: 0.5790
Training set score: 0.8692
Test set score: 0.8475

Naive Bayes Accuracy: 0.7868
Naive Bayes Precision: 0.3740
Naive Bayes Recall: 0.5444
Naive Bayes F1: 0.4434
Naive Bayes AUC: 0.6880
Training set score: 0.7890
Test set score: 0.7868

Decision Tree Accuracy: 0.8250
Decision Tree Precision: 0.3830
Decision Tree Recall: 0.2000
Decision Tree F1: 0.2628
Decision Tree AUC: 0.5702
Training set score: 0.8707
Test set score: 0.8250

k-Nearest Neighbors Accuracy: 0.8319
k-Nearest Neighbors Precision: 0.4186
k-Nearest Neighbors Recall: 0.2000
k-Nearest Neighbors F1: 0.2707
k-Nearest Neighbors AUC: 0.5743
Training set score: 0.8804
Test set score: 0.8319

Support Vector Machines Accuracy: 0.8475
Support Vector Machines Precision: 0.5385
Support Vector Machines Recall: 0.1556
Support Vector Machines F1: 0.2414
Support Vector Machines AUC: 0.5655
Training set score: 0.8975
Test set score: 0.8475

Introduction of Oversampling using SMOTE for all algorithms + 
Add All Features except for ID. 
+ Use Recall as metric for hyperparameters
NOTE: SVM is commented out for now. Ambagal eh
Logistic Regression Accuracy: 0.7470
Logistic Regression Precision: 0.3427
Logistic Regression Recall: 0.6778
Logistic Regression F1: 0.4552
Logistic Regression AUC: 0.7188
Training set score: 0.7087
Test set score: 0.7470

Naive Bayes Accuracy: 0.7227
Naive Bayes Precision: 0.2866
Naive Bayes Recall: 0.5222
Naive Bayes F1: 0.3701
Naive Bayes AUC: 0.6410
Training set score: 0.7400
Test set score: 0.7227

Decision Tree Accuracy: 0.7851
Decision Tree Precision: 0.3229
Decision Tree Recall: 0.3444
Decision Tree F1: 0.3333
Decision Tree AUC: 0.6055
Training set score: 0.9948
Test set score: 0.7851

k-Nearest Neighbors Accuracy: 0.6603
k-Nearest Neighbors Precision: 0.2323
k-Nearest Neighbors Recall: 0.5111
k-Nearest Neighbors F1: 0.3194
k-Nearest Neighbors AUC: 0.5995
Training set score: 0.9970
Test set score: 0.6603

SVM: NaN

Added new features Customer_Enrollmaent and Average Purchase NOTE: SVM IS DEFAULT
Logistic Regression Accuracy: 0.6863
Logistic Regression Precision: 0.2802
Logistic Regression Recall: 0.6444
Logistic Regression F1: 0.3906
Logistic Regression AUC: 0.6692
Training set score: 0.7430
Test set score: 0.6863

Naive Bayes Accuracy: 0.7140
Naive Bayes Precision: 0.2754
Naive Bayes Recall: 0.5111
Naive Bayes F1: 0.3580
Naive Bayes AUC: 0.6313
Training set score: 0.7407
Test set score: 0.7140

Decision Tree Accuracy: 0.7730
Decision Tree Precision: 0.2796
Decision Tree Recall: 0.2889
Decision Tree F1: 0.2842
Decision Tree AUC: 0.5757
Training set score: 0.9948
Test set score: 0.7730

k-Nearest Neighbors Accuracy: 0.6707
k-Nearest Neighbors Precision: 0.2222
k-Nearest Neighbors Recall: 0.4444
k-Nearest Neighbors F1: 0.2963
k-Nearest Neighbors AUC: 0.5785
Training set score: 0.9970
Test set score: 0.6707

Support Vector Machines Accuracy: 0.7435
Support Vector Machines Precision: 0.2836
Support Vector Machines Recall: 0.4222
Support Vector Machines F1: 0.3393
Support Vector Machines AUC: 0.6125
Training set score: 0.6326
Test set score: 0.7435

Try RandomOverSampler and new way of declaring X
Logistic Regression Accuracy: 0.6863
Logistic Regression Precision: 0.2844
Logistic Regression Recall: 0.6667
Logistic Regression F1: 0.3987
Logistic Regression AUC: 0.6783
Training set score: 0.7235
Test set score: 0.6863

Naive Bayes Accuracy: 0.7140
Naive Bayes Precision: 0.2754
Naive Bayes Recall: 0.5111
Naive Bayes F1: 0.3580
Naive Bayes AUC: 0.6313
Training set score: 0.7407
Test set score: 0.7140

Decision Tree Accuracy: 0.8146
Decision Tree Precision: 0.3692
Decision Tree Recall: 0.2667
Decision Tree F1: 0.3097
Decision Tree AUC: 0.5912
Training set score: 0.9525
Test set score: 0.8146

k-Nearest Neighbors Accuracy: 0.7192
k-Nearest Neighbors Precision: 0.2273
k-Nearest Neighbors Recall: 0.3333
k-Nearest Neighbors F1: 0.2703
k-Nearest Neighbors AUC: 0.5619
Training set score: 0.9970
Test set score: 0.7192

Support Vector Machines Accuracy: 0.7539
Support Vector Machines Precision: 0.2969
Support Vector Machines Recall: 0.4222
Support Vector Machines F1: 0.3486
Support Vector Machines AUC: 0.6187
Training set score: 0.6274
Test set score: 0.7539


Go back to SMOTE, random_state = 42, and adjust hyperparameters to scoring = precision. Also introduced SMOTE to DecisionTrees
Also removed tons of features. and simplest optimization for SVM
X = features.drop(columns=['Response', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 
    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 
    'NumWebVisitsMonth', 'Complain'], axis=1)[[
    'Income', 'Age', 'Customer_Enrollment', 'Household', 'Total_Spent', 'Total_Purchases', 
    'Total_Visits', 'Average_Purchase', 'Education_2n Cycle', 'Education_Basic', 'Education_Graduation', 
    'Education_Master', 'Education_PhD', 'Marital_Divorced', 'Marital_Married', 'Marital_Single', 'Marital_Together', 'Marital_Widow'
]]

Logistic Regression Accuracy: 0.7140
Logistic Regression Precision: 0.2843
Logistic Regression Recall: 0.7000
Logistic Regression F1: 0.4043
Logistic Regression AUC: 0.7081
Training set score: 0.7281
Test set score: 0.7140

Naive Bayes Accuracy: 0.7660
Naive Bayes Precision: 0.3050
Naive Bayes Recall: 0.5375
Naive Bayes F1: 0.3891
Naive Bayes AUC: 0.6702
Training set score: 0.7600
Test set score: 0.7660

Decision Tree Accuracy: 0.7972
Decision Tree Precision: 0.3131
Decision Tree Recall: 0.3875
Decision Tree F1: 0.3464
Decision Tree AUC: 0.6253
Training set score: 0.9447
Test set score: 0.7972

k-Nearest Neighbors Accuracy: 0.6950
k-Nearest Neighbors Precision: 0.2333
k-Nearest Neighbors Recall: 0.5250
k-Nearest Neighbors F1: 0.3231
k-Nearest Neighbors AUC: 0.6237
Training set score: 0.9965
Test set score: 0.6950

Support Vector Machines Accuracy: 0.7712
Support Vector Machines Precision: 0.2797
Support Vector Machines Recall: 0.4125
Support Vector Machines F1: 0.3333
Support Vector Machines AUC: 0.6207
Training set score: 0.6303
Test set score: 0.7712

Added back all features plus updated overfitting metric to precision
Logistic Regression Accuracy: 0.7123
Logistic Regression Precision: 0.2871
Logistic Regression Recall: 0.7250
Logistic Regression F1: 0.4113
Logistic Regression AUC: 0.7176
Training set score: 0.7416
Test set score: 0.2871

Naive Bayes Accuracy: 0.7279
Naive Bayes Precision: 0.2579
Naive Bayes Recall: 0.5125
Naive Bayes F1: 0.3431
Naive Bayes AUC: 0.6375
Training set score: 0.7081
Test set score: 0.2579

Decision Trees Accuracy: 0.8198
Decision Trees Precision: 0.3983
Decision Trees Recall: 0.5875
Decision Trees F1: 0.4747
Decision Trees AUC: 0.7223
Training set score: 0.9945
Test set score: 0.3983

k-Nearest Neighbors Accuracy: 0.7123
k-Nearest Neighbors Precision: 0.2584
k-Nearest Neighbors Recall: 0.5750
k-Nearest Neighbors F1: 0.3566
k-Nearest Neighbors AUC: 0.6547
Training set score: 1.0000
Test set score: 0.2584

Support Vector Machines Accuracy: 0.7695
Support Vector Machines Precision: 0.2773
Support Vector Machines Recall: 0.4125
Support Vector Machines F1: 0.3317
Support Vector Machines AUC: 0.6197
Training set score: 0.7117
Test set score: 0.2773

Try RandomOverSampler
Logistic Regression Accuracy: 0.7036
Logistic Regression Precision: 0.2802
Logistic Regression Recall: 0.7250
Logistic Regression F1: 0.4042
Logistic Regression AUC: 0.7126
Training set score: 0.7120
Test set score: 0.2802

Naive Bayes Accuracy: 0.7036
Naive Bayes Precision: 0.2458
Naive Bayes Recall: 0.5500
Naive Bayes F1: 0.3398
Naive Bayes AUC: 0.6392
Training set score: 0.6738
Test set score: 0.2458

Decision Trees Accuracy: 0.8198
Decision Trees Precision: 0.3667
Decision Trees Recall: 0.4125
Decision Trees F1: 0.3882
Decision Trees AUC: 0.6489
Training set score: 0.9930
Test set score: 0.3667

k-Nearest Neighbors Accuracy: 0.7504
k-Nearest Neighbors Precision: 0.2714
k-Nearest Neighbors Recall: 0.4750
k-Nearest Neighbors F1: 0.3455
k-Nearest Neighbors AUC: 0.6349
Training set score: 0.9930
Test set score: 0.2714

Support Vector Machines Accuracy: 0.7626
Support Vector Machines Precision: 0.2683
Support Vector Machines Recall: 0.4125
Support Vector Machines F1: 0.3251
Support Vector Machines AUC: 0.6157
Training set score: 0.6880
Test set score: 0.2683

Introduce Scaling after Oversampling for LogReg, kNN and SVM
Logistic Regression Accuracy: 0.7574
Logistic Regression Precision: 0.3370
Logistic Regression Recall: 0.7750
Logistic Regression F1: 0.4697
Logistic Regression AUC: 0.7648
Training set score: 0.7918
Test set score: 0.3370

Naive Bayes Accuracy: 0.7036
Naive Bayes Precision: 0.2458
Naive Bayes Recall: 0.5500
Naive Bayes F1: 0.3398
Naive Bayes AUC: 0.6392
Training set score: 0.6738
Test set score: 0.2458

Decision Trees Accuracy: 0.8042
Decision Trees Precision: 0.3263
Decision Trees Recall: 0.3875
Decision Trees F1: 0.3543
Decision Trees AUC: 0.6294
Training set score: 0.9904
Test set score: 0.3263

k-Nearest Neighbors Accuracy: 0.7938
k-Nearest Neighbors Precision: 0.3511
k-Nearest Neighbors Recall: 0.5750
k-Nearest Neighbors F1: 0.4360
k-Nearest Neighbors AUC: 0.7020
Training set score: 0.9930
Test set score: 0.3511

Support Vector Machines Accuracy: 0.8527
Support Vector Machines Precision: 0.4590
Support Vector Machines Recall: 0.3500
Support Vector Machines F1: 0.3972
Support Vector Machines AUC: 0.6418
Training set score: 0.9922
Test set score: 0.4590

Reintroduce SMOTE
Logistic Regression Accuracy: 0.8925
Logistic Regression Precision: 0.6607
Logistic Regression Recall: 0.4625
Logistic Regression F1: 0.5441
Logistic Regression AUC: 0.7121
Training set score: 0.9548
Test set score: 0.6607

Naive Bayes Accuracy: 0.7279
Naive Bayes Precision: 0.2579
Naive Bayes Recall: 0.5125
Naive Bayes F1: 0.3431
Naive Bayes AUC: 0.6375
Training set score: 0.7081
Test set score: 0.2579

Decision Trees Accuracy: 0.8198
Decision Trees Precision: 0.3983
Decision Trees Recall: 0.5875
Decision Trees F1: 0.4747
Decision Trees AUC: 0.7223
Training set score: 0.9786
Test set score: 0.3983

k-Nearest Neighbors Accuracy: 0.8250
k-Nearest Neighbors Precision: 0.4019
k-Nearest Neighbors Recall: 0.5375
k-Nearest Neighbors F1: 0.4599
k-Nearest Neighbors AUC: 0.7044
Training set score: 0.9362
Test set score: 0.4019

Support Vector Machines Accuracy: 0.8977
Support Vector Machines Precision: 0.6842
Support Vector Machines Recall: 0.4875
Support Vector Machines F1: 0.5693
Support Vector Machines AUC: 0.7256
Training set score: 0.9544
Test set score: 0.6842

X = features.drop(columns=['Response'], axis=1).iloc[:,1:][['Complain', 
                                                 'Age', 'Customer_Enrollment', 'Household', 'Total_Spent', 
                                                 'Total_Purchases', 'Total_Visits', 'Average_Purchase', 
                                                 'Education_2n Cycle', 'Education_Basic', 'Education_Graduation', 
                                                 'Education_Master', 'Education_PhD', 'Marital_Divorced', 
                                                 'Marital_Married', 'Marital_Single', 'Marital_Together', 'Marital_Widow',
                                                 'Income']]

Logistic Regression Accuracy: 0.8631
Logistic Regression Precision: 0.5091
Logistic Regression Recall: 0.3500
Logistic Regression F1: 0.4148
Logistic Regression AUC: 0.6478
Training set score: 0.9351
Test set score: 0.5091

Naive Bayes Accuracy: 0.7418
Naive Bayes Precision: 0.2745
Naive Bayes Recall: 0.5250
Naive Bayes F1: 0.3605
Naive Bayes AUC: 0.6508
Training set score: 0.7260
Test set score: 0.2745

Decision Trees Accuracy: 0.7903
Decision Trees Precision: 0.3186
Decision Trees Recall: 0.4500
Decision Trees F1: 0.3731
Decision Trees AUC: 0.6475
Training set score: 0.9593
Test set score: 0.3186

k-Nearest Neighbors Accuracy: 0.8284
k-Nearest Neighbors Precision: 0.3733
k-Nearest Neighbors Recall: 0.3500
k-Nearest Neighbors F1: 0.3613
k-Nearest Neighbors AUC: 0.6277
Training set score: 1.0000
Test set score: 0.3733

Support Vector Machines Accuracy: 0.8752
Support Vector Machines Precision: 0.6250
Support Vector Machines Recall: 0.2500
Support Vector Machines F1: 0.3571
Support Vector Machines AUC: 0.6129
Training set score: 0.9831
Test set score: 0.6250







