1st Phase: All Data Cleaning done with RobustScaler as scaler for all but DecisionTrees
List of hyperparameters
Scalers: 0.2 Test Size, 0 Random state
Logistic Regression: solver = liblinear
Niave Bayes: none
DecisionTrees: criterion = entropy, max_depth = 3, RandomState = 0
kNN: neighbors = 3
SVM: C=1.0, kernel=rbf and gamma=auto, default parameters

Logistic Regression Accuracy: 0.8900
Logistic Regression Precision: 0.5714
Logistic Regression Recall: 0.2553
Logistic Regression F1: 0.3529
Logistic Regression AUC: 0.6149
Training set score: 0.8899
Test set score: 0.8900

Naive Bayes Accuracy: 0.7325
Naive Bayes Precision: 0.2222
Naive Bayes Recall: 0.5106
Naive Bayes F1: 0.3097
Naive Bayes AUC: 0.6363
Training set score: 0.7716
Test set score: 0.7325

Decision Trees Accuracy: 0.8725
Decision Trees Precision: 0.1667
Decision Trees Recall: 0.0213
Decision Trees F1: 0.0377
Decision Trees AUC: 0.5036
Training set score: 0.8723
Test set score: 0.8725

K-Nearest Neighbors Accuracy: 0.8700
K-Nearest Neighbors Precision: 0.3810
K-Nearest Neighbors Recall: 0.1702
K-Nearest Neighbors F1: 0.2353
K-Nearest Neighbors AUC: 0.5667
Training set score: 0.9193
Test set score: 0.8700

SVM Accuracy: 0.8875
SVM Precision: 0.6667
SVM Recall: 0.0851
SVM F1: 0.1509
SVM AUC: 0.5397
Training set score: 0.8949
Test set score: 0.8875

--------------------------------------------------------------------------------------------------------------------------------------
StandardScaler - GridSearchCV for best hyperparameters: Logistic Regression, SVM, kNN, and Decision Trees
Features: [Income, Household, Total_Spent, Total_Purchases, Total_Visits, Year of Enrollment, Education, Marital Status]
Logistic Regression Accuracy: 0.8475
Logistic Regression Precision: 0.5312
Logistic Regression Recall: 0.1889
Logistic Regression F1: 0.2787
Logistic Regression AUC: 0.5790
Training set score: 0.8692
Test set score: 0.8475

Naive Bayes Accuracy: 0.7868
Naive Bayes Precision: 0.3740
Naive Bayes Recall: 0.5444
Naive Bayes F1: 0.4434
Naive Bayes AUC: 0.6880
Training set score: 0.7890
Test set score: 0.7868

Decision Tree Accuracy: 0.8250
Decision Tree Precision: 0.3830
Decision Tree Recall: 0.2000
Decision Tree F1: 0.2628
Decision Tree AUC: 0.5702
Training set score: 0.8707
Test set score: 0.8250

k-Nearest Neighbors Accuracy: 0.8319
k-Nearest Neighbors Precision: 0.4186
k-Nearest Neighbors Recall: 0.2000
k-Nearest Neighbors F1: 0.2707
k-Nearest Neighbors AUC: 0.5743
Training set score: 0.8804
Test set score: 0.8319

Support Vector Machines Accuracy: 0.8475
Support Vector Machines Precision: 0.5385
Support Vector Machines Recall: 0.1556
Support Vector Machines F1: 0.2414
Support Vector Machines AUC: 0.5655
Training set score: 0.8975
Test set score: 0.8475

Introduction of Oversampling using SMOTE for all algorithms + 
Add All Features except for ID. 
+ Use Recall as metric for hyperparameters
NOTE: SVM is commented out for now. Ambagal eh
Logistic Regression Accuracy: 0.7470
Logistic Regression Precision: 0.3427
Logistic Regression Recall: 0.6778
Logistic Regression F1: 0.4552
Logistic Regression AUC: 0.7188
Training set score: 0.7087
Test set score: 0.7470

Naive Bayes Accuracy: 0.7227
Naive Bayes Precision: 0.2866
Naive Bayes Recall: 0.5222
Naive Bayes F1: 0.3701
Naive Bayes AUC: 0.6410
Training set score: 0.7400
Test set score: 0.7227

Decision Tree Accuracy: 0.7851
Decision Tree Precision: 0.3229
Decision Tree Recall: 0.3444
Decision Tree F1: 0.3333
Decision Tree AUC: 0.6055
Training set score: 0.9948
Test set score: 0.7851

k-Nearest Neighbors Accuracy: 0.6603
k-Nearest Neighbors Precision: 0.2323
k-Nearest Neighbors Recall: 0.5111
k-Nearest Neighbors F1: 0.3194
k-Nearest Neighbors AUC: 0.5995
Training set score: 0.9970
Test set score: 0.6603

SVM: NaN

Added new features Customer_Enrollmaent and Average Purchase NOTE: SVM IS DEFAULT
Logistic Regression Accuracy: 0.6863
Logistic Regression Precision: 0.2802
Logistic Regression Recall: 0.6444
Logistic Regression F1: 0.3906
Logistic Regression AUC: 0.6692
Training set score: 0.7430
Test set score: 0.6863

Naive Bayes Accuracy: 0.7140
Naive Bayes Precision: 0.2754
Naive Bayes Recall: 0.5111
Naive Bayes F1: 0.3580
Naive Bayes AUC: 0.6313
Training set score: 0.7407
Test set score: 0.7140

Decision Tree Accuracy: 0.7730
Decision Tree Precision: 0.2796
Decision Tree Recall: 0.2889
Decision Tree F1: 0.2842
Decision Tree AUC: 0.5757
Training set score: 0.9948
Test set score: 0.7730

k-Nearest Neighbors Accuracy: 0.6707
k-Nearest Neighbors Precision: 0.2222
k-Nearest Neighbors Recall: 0.4444
k-Nearest Neighbors F1: 0.2963
k-Nearest Neighbors AUC: 0.5785
Training set score: 0.9970
Test set score: 0.6707

Support Vector Machines Accuracy: 0.7435
Support Vector Machines Precision: 0.2836
Support Vector Machines Recall: 0.4222
Support Vector Machines F1: 0.3393
Support Vector Machines AUC: 0.6125
Training set score: 0.6326
Test set score: 0.7435

Try RandomOverSampler and new way of declaring X
Logistic Regression Accuracy: 0.6863
Logistic Regression Precision: 0.2844
Logistic Regression Recall: 0.6667
Logistic Regression F1: 0.3987
Logistic Regression AUC: 0.6783
Training set score: 0.7235
Test set score: 0.6863

Naive Bayes Accuracy: 0.7140
Naive Bayes Precision: 0.2754
Naive Bayes Recall: 0.5111
Naive Bayes F1: 0.3580
Naive Bayes AUC: 0.6313
Training set score: 0.7407
Test set score: 0.7140

Decision Tree Accuracy: 0.8146
Decision Tree Precision: 0.3692
Decision Tree Recall: 0.2667
Decision Tree F1: 0.3097
Decision Tree AUC: 0.5912
Training set score: 0.9525
Test set score: 0.8146

k-Nearest Neighbors Accuracy: 0.7192
k-Nearest Neighbors Precision: 0.2273
k-Nearest Neighbors Recall: 0.3333
k-Nearest Neighbors F1: 0.2703
k-Nearest Neighbors AUC: 0.5619
Training set score: 0.9970
Test set score: 0.7192

Support Vector Machines Accuracy: 0.7539
Support Vector Machines Precision: 0.2969
Support Vector Machines Recall: 0.4222
Support Vector Machines F1: 0.3486
Support Vector Machines AUC: 0.6187
Training set score: 0.6274
Test set score: 0.7539




